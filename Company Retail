package Retail1;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


//use retail data D11,D12,D01 and D02


public class HighValueTranx1{

	
	
	   public static class HighValueMapperClass extends Mapper<LongWritable,Text,Text,Text>
	   {
	      public void map(LongWritable key, Text value, Context context)
	      {
	         try{
	            String[] str = value.toString().split(";");
	            String mykey = "all";  //to group all items under one category since we need grandtotal of entire category and not based on each custid
	            String dt = str[0];
	            String custid = str[1].trim(); //trim used to avoid the space between values in each row of records
	            String sales = str[8];
	            String myValue = dt + ',' + custid + ',' + sales;
	            context.write(new Text(mykey), new Text(myValue));
	         }
	         catch(Exception e)
	         {
	            System.out.println(e.getMessage());
	         }
	      }
	   }

	   //Reducer class
		
	   public static class HighValueReducerClass extends Reducer<Text,Text,NullWritable,Text>
	   {

	      public void reduce(Text key, Iterable <Text> values, Context context) throws IOException, InterruptedException
	      {
	         long maxvalue = 0;
	         String custid = "";
	         String dt = "";

	         for (Text val : values)
	         {
	        	 String[] token = val.toString().split(",");
	        	 if (Long.parseLong(token[2]) > maxvalue)
	        	 {

	        		 maxvalue = Long.parseLong(token[2]);
	        		 dt = token[0];
	        		 custid = token[1];
	        	 }
	         }
	        String myMaxValue = String.format("%d", maxvalue);  //like the printf i.e converting the long datatype back to string datatype
	        String myValue = dt + ',' + custid + ',' + myMaxValue;
			
			context.write(NullWritable.get(), new Text(myValue));
	      }
      
	   }

//Main class
	   
	   public static void main(String[] args) throws Exception {
			
			Configuration conf = new Configuration();  // setting up configuration class
			Job job = new Job(conf, "Highest value single tranx");  // set up the job
		    job.setJarByClass(HighValueTranx1.class);  //setting up the main class to jar
		    job.setMapperClass(HighValueMapperClass.class); // setting up the mapper class. 
		    job.setReducerClass(HighValueReducerClass.class);//setting up the reducer class.If we want to check the mapper o/p only then comment this line
		    //job.setNumReduceTasks(0); //setting up the number of reducers we want as output.If we want to check the mapper o/p only then comment this line
		    job.setMapOutputKeyClass(Text.class);
		    job.setMapOutputValueClass(Text.class);
		    job.setOutputKeyClass(NullWritable.class);
		    job.setOutputValueClass(Text.class);
		    FileInputFormat.addInputPath(job, new Path(args[0]));//setting up file input path
		    FileOutputFormat.setOutputPath(job, new Path(args[1]));//setting up file output path
		    System.exit(job.waitForCompletion(true) ? 0 : 1);
		  }
}
